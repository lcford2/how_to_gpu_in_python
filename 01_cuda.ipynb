{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6f6513",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "First, let's address the most important question -- what is CUDA? Is it a programming language? A driver? A hardware architecture? CUDA stands for **Compute Unified Device Architecture**. In its first [Programming Guide][cuda-1.0], NVIDIA described it as a new type of hardware and software architecture for performing general-purpose computations on NVIDIA GPUs without having to use a graphics API.\n",
    "\n",
    "Today, CUDA ecosystem encompases programming, execution, and memory models, a rich collection of GPU-accelerated libraries, a debugger, a profiler, a compiler, a driver, a toolkit, an integrated development environment, extensions to programming languages such as C and Fortran (called CUDA C and CUDA Fortran), and other tools and know-hows. It is not uncommon to hear people say \"CUDA Programming\" when they refer to programming in CUDA C or CUDA Fortran, GPGPU programming on NVIDIA GPUs, and even GPGPU programming in general.\n",
    "\n",
    "Going back to the original question, we can say that CUDA is an all-encompassing solution that enables us to program NVIDIA graphics cards the same way we program CPUs.\n",
    "In this guide, we will review the programming concepts and hardware details which are essential to CUDA Programming. This information will help you better understand CUDA-oriented Python packages.\n",
    "\n",
    "[cuda-1.0]: https://developer.download.nvidia.com/compute/cuda/1.0/NVIDIA_CUDA_Programming_Guide_1.0.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3aebf9-fe1e-4151-994b-65be487a7408",
   "metadata": {},
   "source": [
    "## CUDA's Model for Parallelism\n",
    "\n",
    "CUDA (and GPUs in general) excel at SIMD parallelism. \n",
    "Basically, wherever you are using a loop or a vectorized function to compute something over a fixed set of data you can leverage the computing power of GPUs.\n",
    "The CUDA programming hiearchy:\n",
    "- Rather than iterating with a index variable, use **threads** and a thread id.\n",
    "  - In CUDA terms, you would write the body of that loop in a function called a **kernel** that is compiled to be called from the host and run on the device.\n",
    "- These threads are organized into **blocks**\n",
    "- The blocks are then divided into **warps** whose size equals the number of cores in a **streaming multiprocessor** (SM)\n",
    "  - A streaming multiprocessor's are collections of **cores** where each **core** contains a arithemetic logic unit (ALU) and a floating point unit (FPU)'\n",
    "  - The entire CUDA architecture is built around these SMs\n",
    "- Warps are then sent to SMs for execution.\n",
    "\n",
    "![CUDA SM Architecture](./figures/automatic-scalability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66e691-474e-46d3-bebb-99ead760e1fc",
   "metadata": {},
   "source": [
    "```C++\n",
    "#include <stdio.h>\n",
    "#define N 64\n",
    "#define TPB 32\n",
    "\n",
    "__device__ float scale(int i, int n) {\n",
    "    return ((float) i)/(n-1);\n",
    "}\n",
    "\n",
    "__device__ float distance(float x1, x2) {\n",
    "    return sqrt((x2 - x1) * (x2 - x1));\n",
    "}\n",
    "\n",
    "__global__ void distanceKernel(float *d_out, float ref, int len) {\n",
    "    const int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    const float x = scale(i, len);\n",
    "    d_out[i] = distance(x, ref);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const float ref = 0.5f;\n",
    "    \n",
    "    // pointer for array of floats\n",
    "    float *d_out = 0;\n",
    "    \n",
    "    // allocate device memory for output array\n",
    "    cudaMalloc(&d_out, N*sizeof(float));\n",
    "    \n",
    "    // launch kernel (1d)\n",
    "    distanceKernel<<<N/TPB, TPB>>>(d_out, ref, N);\n",
    "    \n",
    "    cudaFree(d_out);\n",
    "    return 0;\n",
    "}\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab447691",
   "metadata": {},
   "source": [
    "### Prerequisites and requirements\n",
    "\n",
    "1. Basic knowledge of:\n",
    "    * Unix shell\n",
    "    * C programming language\n",
    "1. Access to a Linux machine with:\n",
    "    * NVIDIA GPU\n",
    "    * CUDA Toolkit\n",
    "    * NVIDIA C compiler (`nvcc`)\n",
    "1. Navigate to the `files` directory of this repository and execute:\n",
    "```\n",
    "nvcc get_device_arch.cu -o get_device_arch\n",
    "./get_device_arch\n",
    "```\n",
    "You should see a message that looks like this:\n",
    "\n",
    "    ```\n",
    "    Detected 1 CUDA Capable device(s)\n",
    "    Device 0: \"Tesla K80\"\n",
    "    Please use: sm_37\n",
    "    ```\n",
    "Write down the last line &mdash; we will need it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24108c59",
   "metadata": {},
   "source": [
    "## Hello, CUDA!\n",
    "\n",
    "Let's start by examining a short program that illustrates basic CUDA syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1212183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file hello_cuda.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void hello_cuda(void) {\n",
    "    printf(\"Hello, CUDA!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Hello, world!\\n\");\n",
    "    \n",
    "    hello_cuda <<<1, 8>>>();\n",
    "    cudaDeviceReset();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bea238",
   "metadata": {},
   "source": [
    "In this program we display text to the screen: one message from CPU, and a few messages from the GPU. Let's save this program to the `hello_cuda.cu` file: the `.cu` extension is commonly used for files that contains CUDA source code. \n",
    "\n",
    "To compile the program, we need the parameter we determined using `get_device_arch` program. In our example above, the value we're talking about is `sm_37`. `sm` stands for \"Streaming multiprocessor\" and we will talk about it later.\n",
    "\n",
    "There is an alternative way to determine an acceptable value for this parameter, which we describe below.\n",
    "\n",
    "<details>\n",
    "    <summary><strong>&#9656;&nbsp;Alternative way to obtain the parameter required for compiling CUDA applications</strong></summary>\n",
    "    \n",
    "An alternative way to look up supported values for the parameter required for compiling CUDA applications is by executing <code>nvcc --list-gpu-code</code> and grabbing the first line of the output:\n",
    "    \n",
    "```\n",
    "$ nvcc --list-gpu-code\n",
    "sm_35\n",
    "sm_37\n",
    "sm_50\n",
    "sm_52\n",
    "sm_53\n",
    "sm_60\n",
    "sm_61\n",
    "sm_62\n",
    "sm_70\n",
    "sm_72\n",
    "sm_75\n",
    "sm_80\n",
    "sm_86\n",
    "```\n",
    "In this case, we write down the first value: `sm_35`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d7815",
   "metadata": {},
   "source": [
    "To compile the program, we use the NVIDIA C compiler (`nvcc`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd058bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch sm_35 hello_cuda.cu -o hello_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1341c1",
   "metadata": {},
   "source": [
    "Now we can execute the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe510b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./hello_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a37383",
   "metadata": {},
   "source": [
    "```\n",
    "Hello, world!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257b5e7",
   "metadata": {},
   "source": [
    "Excellent, we see one \"Hello, world!\" and eight \"Hello, CUDA!\" messages. Let's have a closer look at the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703daa7",
   "metadata": {},
   "source": [
    "### Program breakdown\n",
    "\n",
    "**1. CUDA kernel syntax**<br />\n",
    "The \"Hello, CUDA!\" program features our first CUDA kernel &mdash; `hello_cuda`:\n",
    "\n",
    "```c\n",
    "__global__ void hello_cuda(void) {\n",
    "    printf(\"Hello, CUDA!\\n\");\n",
    "}\n",
    "```\n",
    "\n",
    "It takes no argumets as all it does is display \"Hello, CUDA!\" to the screen. Overall, it looks very similar to a regular C function that displays a single message. The only difference is the `__global__` qualifier. This qualifier tells the compiler that the `hello_cuda` is not a regular function but a CUDA kernel, which is a function that is called from the CPU but executed on a GPU.\n",
    "\n",
    "**2. Calling CUDA kernel**<br />\n",
    "The next interesting part is the way we launch this kernel from the `main` function:\n",
    "\n",
    "```c\n",
    "hello_cuda <<<1, 8>>>();\n",
    "```\n",
    "\n",
    "The triple angle brackets speficy execution parameters. In this particular example, we tell the compiler to execute `hello_cuda` kernel using 8 threads. We will talk about the meaning of these numbers in the following sections.\n",
    "\n",
    "**3. Cleaning up**<br />\n",
    "Finally, we clean up all GPU the resources by calling the `cudaDeviceReset()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d035734",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Remove `cudaDeviceReset` function call from `hello_cuda.cu`, then compile and run the program again.\n",
    "2. Replace `cudaDeviceReset` function call with a call to `cudaDeviceSynchronize`, then compile and run the program again.\n",
    "3. Try compiling the program without the `-arch sm_35` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771ceeb",
   "metadata": {},
   "source": [
    "## CUDA Programming model\n",
    "\n",
    "In the \"Hello, CUDA!\" example we wrote our first **CUDA kernel** and executed it using eight **CUDA threads**. Both of these are fundamental elements of the **CUDA programming model** that describes how elements of a CUDA program use  the underlying NVIDIA hardware (_i.e._, the graphics card).\n",
    "\n",
    "**(Some) Elements of the CUDA programming model**\n",
    "* CUDA kernel\n",
    "    * Execution space qualifiers\n",
    "    * Predefined variables. Global index of a thread.\n",
    "    * Execution parameters\n",
    "* Data types\n",
    "* Variables: qualifier, scope, lifespan\n",
    "* CUDA threads\n",
    "    * Hierarchy: threads, blocks, grids\n",
    "    * Management: synchronization, termination\n",
    "* Elements of NVIDIA GPU device architecture\n",
    "    * Streaming Multiprocessors (SMs), warps, caches\n",
    "    * Compute capabilities\n",
    "    * Memory bandwidths\n",
    "* Memory\n",
    "    * Types: register, local, shared, global, constant, pinned, zero-copy\n",
    "    * Management: allocation, deallocation, transfer\n",
    "* CUDA streams and events\n",
    "* Runtime vs Driver API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa3e99",
   "metadata": {},
   "source": [
    "### CUDA kernel: execution space qualifiers\n",
    "\n",
    "A CUDA kernel is a special kind of function that is executed on a GPU device. It is written as a sequential program but executed by multiple CUDA threads each running on a single CUDA core. Conceptually, executing a CUDA kernel is similar to executing a multithreaded function on a CPU:\n",
    "\n",
    "<a href=\"./figures/multithreaded-code-vs-cuda-kernel.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/multithreaded-code-vs-cuda-kernel.jpg\" width=\"700\" alt=\"Involvement of CPU and GPU cores in execution of multithreaded CPU code and CUDA kernel.\" />\n",
    "</a>\n",
    "\n",
    "In the source code, CUDA kernel declaration must be prefixed with an execution space qualifier that instructs CUDA where the function is called and where it is executed. In the \"Hello, CUDA\" example we saw one such qualifier &ndash; `__global__`, which tells the compiler that the function will be executed on a GPU but called from a CPU (with some exceptions that we omit in this brief explanation for brevity and clarity). There are two other execution space qualifiers, `__device__` and `__host__`:\n",
    "\n",
    "1. `__device__`: function is executed on and called from a GPU.\n",
    "2. `__host__`: function is executed on and called from a CPU.\n",
    "\n",
    "The `__global__` qualifier can not be combined with the other ones, but `__device__` and `__host__` can be specified together. This is useful when one needs the same functionality on the host and device and wants to reduce code duplication.\n",
    "\n",
    "<a href=\"./figures/global_device_host.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/global_device_host.jpg\" width=\"700\" alt=\"Effect of execution space qualifiers on where the function/kernel can be called and executed.\" />\n",
    "</a>\n",
    "\n",
    "As a quick exercise, let's rewrite the `hello_cuda` kernel so we can call it from CPU and GPU, that is, let's rewrite it as a `__host__ __device__` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file hello_cuda_2.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__host__ __device__ void hello_cuda(void) {\n",
    "#if defined(__CUDA_ARCH__)\n",
    "   printf(\"Hello, CUDA!\\n\");\n",
    "#else\n",
    "   printf(\"Hello, world!\\n\");\n",
    "#endif\n",
    "}\n",
    "\n",
    "__global__ void helper_kernel(void) {\n",
    "    hello_cuda();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    helper_kernel <<<1,8>>>();\n",
    "    hello_cuda();\n",
    "    cudaDeviceReset();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd839386",
   "metadata": {},
   "source": [
    "In this version of the `hello_cuda` kernel we had to use `__CUDA_ARCH__` macro to differentiate CPU and GPU execution environments. We also had to create a \"helper\" CUDA kernel -- `helper_kernel` -- to call the `hello_cuda` device function, because we can't call the device function directly from a CPU.\n",
    "\n",
    "Let's compile and execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch sm_35 hello_cuda_2.cu -o hello_cuda_2\n",
    "!./hello_cuda_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08432eaa",
   "metadata": {},
   "source": [
    "```\n",
    "Hello, world!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7e574",
   "metadata": {},
   "source": [
    "Notice anything out of order? In the function `main`, we call device function before we call the host function. Yet, in the output we see output from the host function first. This happens because **CUDA kernels are asynchronous**, meaning that they return control to main CPU thread immediately and we must take extra steps when we need to ensure proper order of execution. We can achieve this by calling `cudaDeviceSynchronize` function between the calls to device and host functionс:\n",
    "\n",
    "```c\n",
    "int main() {\n",
    "    helper_kernel <<<1,8>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    hello_cuda();\n",
    "    cudaDeviceReset();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Update the main function as shown above, recompile the program and execute it. You should see the following output:\n",
    "\n",
    "```\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, CUDA!\n",
    "Hello, world!\n",
    "```\n",
    "\n",
    "Now, the host function is not called until the device function finished, so messages from the GPU appear before messages from the CPU. Synchronization is an important topic in CUDA programming and we will come back to it once again soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4a82e",
   "metadata": {},
   "source": [
    "### Thread hierarchy and kernel execution parameters  \n",
    "\n",
    "To spread out work across CUDA threads, we need to know how to differentiate them. To do that, we need to know the CUDA thread hierarchy model.\n",
    "\n",
    "When we launch a CUDA kernel, spawned CUDA threads are groupped into blocks which colletively form a _grid_:\n",
    "\n",
    "<a href=\"./figures/thread_hierarchy.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/thread_hierarchy.jpg\" width=\"400\" alt=\"CUDA thread hierarchy model.\" />\n",
    "</a>\n",
    "\n",
    "That's why you may hear about a \"grid of a kernel\" and \"thread blocks\". Within this hierarchy:\n",
    "\n",
    "* All threads in a grid share global memory space.\n",
    "* Threads in a single block:\n",
    "    * can synchronize with each other,\n",
    "    * have access to block-local shared memory.\n",
    "* Threads in different blocks cannot cooperate.\n",
    "\n",
    "The number of blocks in a grid and the number of threads in a block are the two parameters controlled by the programmer at the time a CUDA kernel is launched. These are the execution parameters specified in triple angle brackets (`<<< >>>`).\n",
    "\n",
    "Recall that in the \"Hello, CUDA!\" program we launched our kernel using `hello_cuda <<<1, 8>>>();` call. Specified parameters, `<<<1, 8>>>`, instruct CUDA to execute the kernel using a single block of 8 threads:\n",
    "\n",
    "<a href=\"./figures/hello_cuda_thread_model.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/hello_cuda_thread_model.jpg\" width=\"400\" alt=\"'Hello, CUDA' thread hierarchy model.\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952171f8",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Execute the `hello_cuda` kernel using:\n",
    "\n",
    "   * 2 blocks and 4 threads per block\n",
    "   * 4 blocks and 2 threads per block\n",
    "   * 8 blocks and 1 thread per block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c46e96",
   "metadata": {},
   "source": [
    "### Block and thread arrangement\n",
    "\n",
    "Within the CUDA programming model, blocks in a grid and threads in a block can be arranged in three dimensions. So, in a CUDA kernel call:\n",
    "\n",
    "```c\n",
    "kernel_name <<< A, B >>> (arguments);\n",
    "```\n",
    "\n",
    "`A` can specify not only the number of blocks in a grid but a 3D arrangement of blocks. Similarly, `B` can specify not only the number of threads in a block but a 3D arrangement of threads within a block. For example, a 2$\\times$2$\\times$1 2D grid of blocks where every block has 2 threads arranged in 1D (1D blocks) can be created in the following way:\n",
    "\n",
    "```c\n",
    "// 2D grid, 1D blocks\n",
    "dim3 A(2, 2); // same as: dim3 A(2, 2, 1);\n",
    "dim3 B(2);    // same as: dim3 B(2, 1, 1); \n",
    "kernel_name <<< A, B >>> (arguments);\n",
    "```\n",
    "\n",
    "Here we use a new data struct &mdash; `dim3`. This is a vectors of 3 integers, each of which defaults to `1`. Components of this data structure are `x`, `y`, and `z`.\n",
    "\n",
    "Here is another example, where we create a grid populated by a single block that contains 8 threads in a 2$\\times$2$\\times$2 arrangement:\n",
    "\n",
    "```c\n",
    "// 1D grid, 3D blocks\n",
    "dim3 A; // same as dim3 A(1, 1, 1);\n",
    "dim3 B(2, 2, 2);\n",
    "kernel_name <<< A, B >>> (arguments);\n",
    "```\n",
    "Of course, we could also call the kernel like so:\n",
    "```c\n",
    "kernel_name <<< 1, B >>> (arguments);\n",
    "```\n",
    "which would yield the same arrangement of blocks and threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19d15a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Update `hello_cuda` kernel call execution parameters so that the grid has:\n",
    "\n",
    "1. Two blocks with 4 threads per block in 1D arrangement.\n",
    "2. Two blocks with 4 threads per block in 2$\\times$2$\\times$1 arrangement.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><strong>&#9656;&nbsp;Solutions</strong></summary>\n",
    "        \n",
    "1. \n",
    "```c\n",
    "hello_cuda <<< 2, 4 >>> ();\n",
    "```\n",
    "2.\n",
    "```c\n",
    "dim3 grid(2);\n",
    "dim3 block(2, 2);\n",
    "hello_cuda <<< grid, block >>> ();\n",
    "```\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbb171",
   "metadata": {},
   "source": [
    "### Predefined variables. Global thread index\n",
    "\n",
    "When CUDA thread executes a kernel, it has access to 4 predefined variables:\n",
    "\n",
    "1. `gridDim`: grid dimensions (measured in blocks),\n",
    "2. `blockDim`: block dimensions (measured in threads),\n",
    "3. `blockIdx`: block position within a grid, and\n",
    "4. `threadIdx`: thread position within a block.\n",
    "\n",
    "All of the variables are of a new type `uint3` which is similar to `dim3` except that there are no default values. These variables can be used to calculate _global thread index_ to uniquely identify each thread and, therefore, its workload. In case of 1D arrangement of blocks and threads, global index of a thread can be computed as:\n",
    "\n",
    "```c\n",
    "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "```\n",
    "\n",
    "`idx` is a widely accepted name for the variable that stores global index of a thread. Here is an example of a kernel that is launched with four 1D blocks each having six 1D threads:\n",
    "<a href=\"./figures/global_thread_index.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/global_thread_index.jpg\" width=\"500\" alt=\"Global thread index: 1D example\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d26ce0",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Rewrite the `hello_cuda` kernel so that every thread prints its block, index within the block, and its global thread index.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><strong>&#9656;&nbsp;Solutions</strong></summary>\n",
    "        \n",
    "```c\n",
    "__host__ __device__ void hello_cuda(void) {\n",
    "#if defined(__CUDA_ARCH__)\n",
    "    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    printf(\"Hello from block %d, thread %d, global thread id %d\\n\", blockIdx.x, threadIdx.x, idx);\n",
    "#else\n",
    "    printf(\"Hello, world!\\n\");\n",
    "#endif\n",
    "}\n",
    "```\n",
    "</details>\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70735f",
   "metadata": {},
   "source": [
    "To uniquely identify a thread when using two- and/or three-dimensional arrangement of blocks or threads, one can use either global thread indicies along each direction:\n",
    "\n",
    "```c\n",
    "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "unsigned int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "unsigned int idz = threadIdx.z + blockIdx.z * blockDim.z;\n",
    "```\n",
    "\n",
    "or compute a single global thread index:\n",
    "```c\n",
    "unsigned int blockDim_xy = blockDim.x * blockDim.y;\n",
    "unsigned int blockDim_xyz = blockDim_xy * blockDim.z;\n",
    "unsigned int gridDim_xy = gridDim.x * gridDim.y;\n",
    "unsigned int idx_local = threadIdx.x  + threadIdx.y * blockDim.x  + threadIdx.z * blockDim_xy;\n",
    "unsigned int block_shift =  blockIdx.x  +  blockIdx.y *  gridDim.x  +  blockIdx.z *  gridDim_xy;\n",
    "unsigned int idx = idx_local + block_shift * blockDim_xyz;\n",
    "```\n",
    "Here is an example of applying the above rules for computing 1D global thread index:\n",
    "\n",
    "<a href=\"./figures/global_thread_index_explained.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/global_thread_index_explained.jpg\" width=\"700\" alt=\"Global thread index: 3D example.\" />\n",
    "</a>\n",
    "\n",
    "Keep in mind that there are other ways to calculate global index of a thread: we could traverse blocks and threads in the `z` direction first, or even use different traversal schemes for blocks and threads. However, the best approach takes into account data layout in memory and, typically, matrix-like data are stored linearly with a row-major approach, which corresponds to the traversal approach presented above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f142a74",
   "metadata": {},
   "source": [
    "#### Arrangements of blocks and threads - why do we need it?\n",
    "\n",
    "So, why might we need (or prefer) to use a 2D or 3D arrangement of blocks and threads? The choise of what arrangement of blocks and threads to use is dictated by the input data and convenience. Imagine working with RGB images where every point on a 2D plane provides three values: it is possible to linearize the data into a 1D array and process it using 1D grid of 1D blocks, but it's far more convenient to use 2 separate indicies to uniquely identify each point. The same applies to many other types of data -- 3D distibutions of potentials, vector fields,  and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34521440",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Modify the `hello_cuda` kernel so that each thread prints the block it belongs to, its thread ID, and global thread ID.\n",
    "\n",
    "<details>\n",
    "    <summary><strong>&#9656;&nbsp;Solution</strong></summary>\n",
    "        \n",
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "__host__ __device__ void hello_cuda(void) {\n",
    "#if defined(__CUDA_ARCH__)\n",
    "    unsigned int blockDim_xy = blockDim.x * blockDim.y;\n",
    "    unsigned int blockDim_xyz = blockDim_xy * blockDim.z;\n",
    "    unsigned int gridDim_xy = gridDim.x * gridDim.y;\n",
    "    unsigned int idx_local = threadIdx.x  + threadIdx.y * blockDim.x  + threadIdx.z * blockDim_xy;\n",
    "    unsigned int block_shift =  blockIdx.x  +  blockIdx.y *  gridDim.x  +  blockIdx.z *  gridDim_xy;\n",
    "    unsigned int idx = idx_local + block_shift * blockDim_xyz;\n",
    "    printf(\"blockIdx: (%d, %d, %d)  threadIdx: (%d, %d, %d)  global index: %d\\n\",\n",
    "            blockIdx.x, blockIdx.y, blockIdx.z,\n",
    "            threadIdx.x, threadIdx.y, threadIdx.z,\n",
    "            idx);\n",
    "#else\n",
    "    printf(\"Hello, world!\\n\");\n",
    "#endif\n",
    "}\n",
    "\n",
    "__global__ void helper_kernel(void) {\n",
    "    hello_cuda();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    dim3 grid(1, 2, 3);\n",
    "    dim3 block(1, 2, 3);\n",
    "    helper_kernel <<< grid, block>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    hello_cuda();\n",
    "    cudaDeviceReset();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078838d",
   "metadata": {},
   "source": [
    "### Data / Memory Management\n",
    "\n",
    "So far, we've been developing a naive application that merely prints text to the screen. Understandably, one may want to use GPUs for something more useful, for example, to process or analyze data. Here we have to step back and remind ourselves that CPU-GPU systems are heterogeneous and both CPU and GPU can access their own memory only. This means that in order to process data on the GPU, we have to  transfer it there. Of course, to view or save the results we have to transfer them back to the CPU.\n",
    "\n",
    "CUDA provides two sister functions for transferring data between the host and device: `cudaMemcpy` and `cudaMemcpyAsync`. They work exactly the same way but the latter one is asynchronous and is used in advanced applications when data throughput is of critical importance.\n",
    "\n",
    "Another two important data management functions are:\n",
    "\n",
    "* `cudaMalloc`, which allocates GPU memory, and\n",
    "* `cudaFree`, which releases allocated GPU memory. \n",
    "\n",
    "So let's talk about how these functions enable data analysis on GPU. First, let's have a look at the (simplified) signature of `cudaMemcpy`:\n",
    "\n",
    "```c\n",
    "cudaMemcpy(*destination, *source, nbytes, direction);\n",
    "```\n",
    "\n",
    "Arguments of the `cudaMemcpy` command are:\n",
    "\n",
    "- `*destination`: pointer to the memory location where the data is sent (copied) to\n",
    "- `*source`: pointer to the data being transferred.\n",
    "- `nbytes`: transferred data size in bytes,\n",
    "- `direction`: direction of memory transfer. Possible values are:\n",
    "    * `cudaMemcpyHostToDevice`\n",
    "    * `cudaMemcpyDeviceToHost`\n",
    "    * `cudaMemcpyDeviceToDevice`\n",
    "    * `cudaMemcpyHostToHost`\n",
    "\n",
    "### Four steps of memory management\n",
    "\n",
    "In general, processing data on a GPU device has six major steps, four of which deal with memory management:\n",
    "\n",
    "**Step 1**: Allocate memory on the device (`cudaMalloc`) and host (`malloc`).<br>\n",
    "_Prepare data on the host (initialize, read, generate, etc)_<br>\n",
    "**Step 2**: Transfer data from host to device (`cudaMemcpy`, direction: `cudaMemcpyHostToDevice`)<br>\n",
    "_Process data on the device_<br>\n",
    "**Step 3**. Transfer the results back to host (`cudaMemcpy`, direction: `cudaMemcpyDeviceToHost`)<br>\n",
    "**Step 4**. Free memory on the device (`cudaFree`) and host (`free`).\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```c\n",
    "// define data parameters\n",
    "unsigned int arr_size = 1<<22; \n",
    "unsigned int nbytes = arr_size * sizeof(float);\n",
    "\n",
    "// Allocate host memory\n",
    "float *h_a = (float *)malloc(nbytes);\n",
    "\n",
    "// Allocate device memory\n",
    "float *d_a;\n",
    "cudaMalloc((float**)&d_a, nbytes);\n",
    "\n",
    "// Initialize host memory\n",
    "for (unsigned int i=0; i<arr_size; i++) h_a[i] = 0.5f * i;\n",
    "\n",
    "// Transfer data from host to device\n",
    "//          to<~from                  from~>to\n",
    "cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice);\n",
    "\n",
    "// Process data on the device\n",
    "\n",
    "// Transfer data back from device to host\n",
    "//          to<~from                     from~>to\n",
    "cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "// free memory\n",
    "cudaFree(d_a);\n",
    "free(h_a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e08fd",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a program that multiplies an array of floating point numbers by 2.\n",
    "\n",
    "<details>\n",
    "    <summary><strong>&#9656;&nbsp;Solution</strong></summary>\n",
    "    \n",
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__\n",
    "void multiply(float *in, float *out, const int n) {\n",
    "    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n)\n",
    "        out[idx] = in[idx] * 2.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    unsigned int arr_size = 1<<22; \n",
    "    unsigned int nbytes = arr_size * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_in = (float *)malloc(nbytes);\n",
    "    float *h_out = (float *)malloc(nbytes);\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_in;\n",
    "    float *d_out;\n",
    "    cudaMalloc((float **)&d_in, nbytes);\n",
    "    cudaMalloc((float **)&d_out, nbytes);\n",
    "\n",
    "    // Initialize host memory\n",
    "    for (unsigned int i=0; i<arr_size; i++)\n",
    "        h_in[i] = 0.5f * i;\n",
    "\n",
    "    // Transfer data from host to device\n",
    "    //         to <~ from                    from~>to\n",
    "    cudaMemcpy(d_in, h_in, nbytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Set kernel execution parameters: \n",
    "    // 1. Use 128 threads per block\n",
    "    // 2. Compute the number of necessary blocks\n",
    "    int blocksize = 128;\n",
    "    int gridsize = (arr_size + blocksize - 1) / blocksize;\n",
    "\n",
    "    dim3 grid(gridsize);\n",
    "    dim3 block(blocksize);\n",
    "\n",
    "    multiply <<<grid, block>>>(d_in, d_out, arr_size);\n",
    "    \n",
    "    // Transfer data back from device to host\n",
    "    //          to <~ from                     from~>to\n",
    "    cudaMemcpy(h_out, d_out, nbytes, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // test results\n",
    "    for (unsigned int i=1; i<arr_size; i = i<<1)\n",
    "        printf(\"h_out[%d] = %f\\n\", i, h_out[i]);\n",
    "\n",
    "    // release memory\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    free(h_in);\n",
    "    free(h_out);\n",
    "    \n",
    "    cudaDeviceReset();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa7575",
   "metadata": {},
   "source": [
    "### Memory bandwidths in a CPU-GPU system\n",
    "\n",
    "Efficient memory transfer is an important part of every CUDA application that deals with data. This is due to the intrinsic limitations of the hardware components of heterogeneous CPU--GPU systems. The following diagram shows important memory paths in a CPU-GPU heterogeneous system: \n",
    "\n",
    "<a href=\"./figures/memory_bandwidth.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/memory_bandwidth.jpg\" width=\"600\" />\n",
    "</a>\n",
    "\n",
    "PCIe is the slowest path in a heterogeneous system and as a programmers you should always try to minimize the amount of data transferred using this path or overlap such transfers with other useful work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12886675",
   "metadata": {},
   "source": [
    "### CUDA Memory model\n",
    "\n",
    "Memory access and management are important aspects of the CUDA programming model because improper or inefficient use of memory may lead to degraded performance and bottlenecks. CUDA memory model exposes many types of GPU memory:\n",
    "\n",
    "* Registers\n",
    "* Local memory\n",
    "* Shared memory\n",
    "* Constant memory\n",
    "* Texture memory\n",
    "* Global memory\n",
    "\n",
    "<a href=\"./figures/memory_hierarchy.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/memory_hierarchy.jpg\" width=\"600\" alt=\"CUDA memory model\" />\n",
    "</a>\n",
    "    \n",
    "Each type of memory has its own scope, lifetime, and purpose:\n",
    "- **Registers**: store a limited amount of automatic variables that are declared in a kernel without special qualifiers. Fastest memory space on a GPU. Fermi cards have 63 registers per thread, Kepler cards - 255 registers per thread. Has a lifetime of a kernel (thread).\n",
    "   Example:\n",
    "   ```\n",
    "   float pi = 3.14f;\n",
    "   ```\n",
    "- **Local memory**: for variables declared in a kernel that don't fit into registers. Each thread has its own private local memory. Name is misleading: \"local memory\" resides in the same physical location as Global memory. Has a lifetime of a kernel (thread).\n",
    "   Example:\n",
    "   ```\n",
    "   float data[100];\n",
    "   ```\n",
    "- **Shared memory**: memory within a thread block that is visible to all threads and whose lifetime is the lifetime of the block. Faster (higher bandwidth, lower latency) than local or global memory. Access to shared memory must be synchronized using `__syncthreads()` function that creates a barrier that all threads in the same thread block must reach before any other thread is allowed to proceed. Variables decorated with `__shared__` are stored in shared memory.\n",
    "   Example:\n",
    "   ```\n",
    "   __shared__ float var;\n",
    "   __shared__ float data[100];\n",
    "   ```\n",
    "- **Global memory**: Largest, highest-latency memory that all threads can access. The name _global_ refers to its scope (global) and lifetime (application). Dynamic allocation/deallocation _via_ `cudaMalloc` and `cudaFree` host functions. Variables decorated with `__device__` are stored in the global memory.\n",
    "    ```\n",
    "    __device__ float var;\n",
    "    __device__ float data[100];\n",
    "    ```\n",
    "    Note that to transfer statically declared variables to/from GPU, one has to use `cudaMemcpyToSymbol` and `cudaMemcpyFromSymbol` functions:\n",
    "    ```\n",
    "    // file-scope\n",
    "    __device__ float devVar;\n",
    "    \n",
    "    // in main()\n",
    "    float value = 3.14f;\n",
    "    cudaMemcpyToSymbol(devVar, &value, sizeof(float));\n",
    "    ...\n",
    "    cudaMemcpyFromSymbol(&value, devData, sizeof(float));\n",
    "    ```\n",
    "- **Constant memory** is read-only memory space for statically declared variables (no `cudaMalloc` etc) that are visible to all kernels. Kernels can only read from the constant memory. Must be declared outside of any kernels. Good use case: a coefficient or a constant for a mathematical formula. Not so good use case: when each thread (in a _warp_) reads its own address once.\n",
    "    Example:\n",
    "    ```\n",
    "    __constant__ float var;\n",
    "    __constant__ float data[100];\n",
    "    ```\n",
    "- **Texture memory**: special kind of read-only global memory that can perform floating-point interpolation as part of the read process (\"filtering\").\n",
    "\n",
    "Besides the above types of GPU memory, CUDA memory model also exposes host and hybrid types of memory:\n",
    "\n",
    "- **Pinned memory** is a host memory that the host operating system can not move to another physical location. Also called _page-locked memory_. Allocated on host via `cudaMallocHost` and freed with `cudaFreeHost`. More \"expensive\" to allocate and deallocate, but provides better throughput. Beneficial for sizeable data transfers (10MB of data).\n",
    "- **Zero-copy memory**: pinned host memory that is mapped into the device address space. Both the host and device can access zero-copy memory but memory accesses must be synchronized. OK to use for small amount of data because it simplifies programming. For large datasets, zero-copy memory is a poor choice because it causes significant performance degradation.\n",
    "- **Unified memory**: a pool of managed memory, where each allocation is accessible on both the CPU and GPU with the same memory address. The underlying system automatically migrates data between host and device when necessary. Like _zero-copy memory_, offers a \"single pointer to data\" model but transparently migrates data between host and device as necessary to improve performance.\n",
    "    Example:\n",
    "    ```\n",
    "    __device__ __managed__ float data; // in file-scope or global-scope\n",
    "\n",
    "    cudaMallocManaged(void **devhostPtr, size_t size, int flags=0);\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4c417",
   "metadata": {},
   "source": [
    "## CUDA Streams\n",
    "\n",
    "All CUDA operations, whether explicitly or implicitly, run within the so-called CUDA streams. A CUDA stream is a sequence of CUDA operations executed on a device in the order prescribed by the host code. When CUDA stream is not specified, CUDA operations are performed in the default stream called the NULL stream.\n",
    "\n",
    "CUDA streams enable concurrent execution of multiple independent sequences of CUDA operations, thus improving utilization of the underlying hardware. CUDA streams can be compared to railroad tracks that provide a pathway for the trains &mdash; CUDA operations (kernels, data transfers, etc).\n",
    "\n",
    "CUDA streams can be blocking and non-blocking. Blocking streams can not run in parallel with the default NULL stream but can run in parallel with each other. In the railroads-trains analogy, blocking streams are the railroads that share a common power source with the default NULL stream and non-blocking streams have their own power source.\n",
    "\n",
    "Data transfer operations may not always be executed in parallel even when they're issued in different streams. This is due to the fact that they must share a common resource - the PCIe bus. Only data transfers in opposite directions and in different streams can run in parallel with each other. In the railroads-trains analogy, data transfers can be compared to tunnels with two tracks running in opposite directions and trains that are going in the same direction must share a single railroad track.\n",
    "\n",
    "<a href=\"./figures/streams_railroad_analogy.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/streams_railroad_analogy.jpg\" alt=\"Railroad tracks analogy for CUDA streams\" />\n",
    "</a>\n",
    "\n",
    "### CUDA Stream commands\n",
    "\n",
    "To create CUDA streams, we first need to initialize variables that we will use to refer to them:\n",
    "\n",
    "```\n",
    "// single stream\n",
    "cudaStream_t stream;\n",
    "\n",
    "// multiple streams\n",
    "cudaStream_t *streams = (cudaStream_t *)malloc(n_streams * sizeof(cudaStream_t));\n",
    "```\n",
    "\n",
    "Then, we can create regular blocking streams and non-blocking streams like so:\n",
    "```\n",
    "// blocking stream\n",
    "cudaStreamCreate(&stream); \n",
    "\n",
    "// non-blocking stream\n",
    "cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);\n",
    "```\n",
    "\n",
    "To release resources of a stream (to destroy a stream):\n",
    "```\n",
    "cudaStreamDestroy(stream);\n",
    "```\n",
    "\n",
    "Because all CUDA stream operations are asynchronous, it's important to be able to query a stream about its status and to tell CUDA to wait for a stream to complete its operations. CUDA provides two functions for that:\n",
    "\n",
    "```\n",
    "// block host until operations in the stream have completed\n",
    "cudaStreamSynchronize(stream);\n",
    "\n",
    "//Query the stream about its status. \n",
    "cudaStreamQuery(stream)\n",
    "```\n",
    "\n",
    "To do memory transfer within a stream, we have to use `cudaMemcpyAsync` function:\n",
    "```\n",
    "cudaMemcpyAsync(destination, source, shared_memory_size, stream);\n",
    "```\n",
    "\n",
    "CUDA streams come hand in hand with CUDA events, which are essentially markers of progress in CUDA streams. They, however, fall beyond the scope of this introduction to CUDA and we'd like to encourage our students to read about them in the referenced material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e30d4",
   "metadata": {},
   "source": [
    "### NVIDIA GPU architecture overview\n",
    "\n",
    "Now let's discuss the hardware and how it affects CUDA applications. NVIDIA graphics card that support CUDA are called CUDA-capable or CUDA-enabled. There are several generations of such cards and each generation brings new features and capabilities: ability to display text right from the kernel (i.e., use `printf` function), dynamic parallelism, unified memory, and so on. These capabilities are called _compute capabilities_ and NVIDIA assignes them version numbers, also sometimes called \"SM versions\". This number is used by applications at runtime to determine which hardware features are available on the present GPU device. More information about compute capabilities you can find here: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
    "\n",
    "### SM: Streaming Multiprocessor\n",
    "\n",
    "As we mentioned above, compute capability version is also called \"SM version\". SM stands for Streaming Multiprocessor, which is the fundamental component of a CUDA-enabled NVIDIA GPU. Here is a diagram of one of such GPUs (Kepler K20x):\n",
    "\n",
    "<a href=\"./figures/kepler_k20x.jpg\" target=\"_blank\">\n",
    "<img src=\"./figures/kepler_k20x.jpg\" alt=\"Diagram of a CUDA-enabled NVIDIA GPU (Kepler K20x).\" />\n",
    "</a>\n",
    "\n",
    "As you can see, NVIDIA GPU architecture is built around Streaming Multiprocessors that house all the computing resources: CUDA cores, Load/Store units, Special Function Units, Register file, Shared memory, warps, and etc. Parallelism is achieved by replicating Streaming Multipricessors, so a more powerful GPU can be constructed by increasing the number of these SMs.\n",
    "\n",
    "### SMs, kernels, and grids\n",
    "\n",
    "When a kernel grid is launched, the thread blocks of that kernel grid are distributed among available SMs for execution. Once _scheduled_ on an SM, threads of a thread block _execute_ concurrently only on that assigned SM. Multiple thread blocks may be assigned to the same SM at once and are scheduled based on the availability of SM resources (there is a limit on the number of threads an SM can run concurrently).\n",
    "\n",
    "### Warps and SIMT\n",
    "\n",
    "CUDA employes a Signle Instruction Multiple Thread (SIMT) architecture to manage and execute threads in groups of 32 called warps. All threads in a warp execute the same instruction at the same time. Each SM partitions the thread blocks assigned to it into 32-thread warps that it then schedules for execution on available hardware resources. The SIMT architecture is similar to SIMD except that SIMT allows multiple threads in the same warp to execute independently, whereas SIMD requires synchronous execution.\n",
    "\n",
    "\n",
    "### Magic number: 32\n",
    "\n",
    "As we mentioned above, warps are groups of 32 threads. 32 is a magic number in CUDA programming. Optimiting your workloads to fit within the boundaries of a warp will generally lead to better utilization of GPU resources and, therefore, better performance of your application. That's why programmers often use blocks of 32, 64, 128, and 1024 threads. Note that there is a limit on the number of threads per block -- 1024, which corresponds to 32 warps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22583d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. CUDA Programming Guide: https://docs.nvidia.com/cuda\n",
    "2. \"Professional CUDA C Programming\" by John Cheng, Max Grossman, Ty McKercher. ([wiley.com](https://www.wiley.com/en-us/Professional+CUDA+C+Programming-p-9781118739310), [amazon.com](https://www.amazon.com/dp/1118739329/ref=cm_sw_em_r_mt_dp_GEF22X8H3JJKS51G5107))\n",
    "3. \"GPU Parallel Program Development Using CUDA\" by Tolga Soyata ([routledge.com](https://www.routledge.com/GPU-Parallel-Program-Development-Using-CUDA/Soyata/p/book/9780367572242), [amazon.com](https://www.amazon.com/dp/1498750753/ref=cm_sw_em_r_mt_dp_5KACSGQMH4RZBJG988GB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
