{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b0a020",
   "metadata": {},
   "source": [
    "## CuPy\n",
    "\n",
    "CuPy is a Python package that implements NumPy arrays and methods on GPUs. CuPy positions itself as a GPU-accelerated drop-in replacement for NumPy and SciPy, but does, in fact, provide a lot more than that, including some low-level CUDA support. Note that CuPy primarily targets NVIDIA CUDA-capable devices (hence its name) but does provide experimental support for AMD ROCm devices. \n",
    "\n",
    "So, let's have a look at the basics of using CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793be06",
   "metadata": {},
   "source": [
    "### GPU-accelerated NumPy\n",
    "\n",
    "CuPy can be used as a drop-in GPU-accelerated replacement for Numpy. Using CuPy for this purpose is as easy as going through your Python code and replacing `numpy` with `cupy` (and/or `np` with `cp`), for example, by changing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "h_data = numpy.array([1, 2, 3])\n",
    "h_L2 = numpy.linalg.norm(h_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ec650",
   "metadata": {},
   "source": [
    "to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf730c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "d_data = cupy.array([1, 2, 3])\n",
    "d_L2 = cupy.linalg.norm(d_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e93dd6",
   "metadata": {},
   "source": [
    "In general, CuPy tries to preserve NumPy behavior. However, there are some differences, which are documented here: https://docs.cupy.dev/en/stable/user_guide/difference.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada6ed1",
   "metadata": {},
   "source": [
    "### Data transfer\n",
    "\n",
    "When we merely replace `numpy` with `cupy`, both data and calculations move to GPU. Afterall, this is the whole point of using CuPy. To move data between CPU and GPU, CuPy provides several methods:\n",
    "\n",
    "#### Moving data to GPU\n",
    "\n",
    "* `cupy.asarray` method moves any object that can be passed to `numpy.array` to the currently active GPU. This method accepts CuPy arrays too. This method is similar to the `cupy.array` method we used to create CuPy arrays above.\n",
    "* `cupy.ndarray.set` method sets values of an existing CuPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import numpy\n",
    "\n",
    "d_array = cupy.asarray([1, 2, 3])\n",
    "n = 3\n",
    "d_a = cupy.empty((n, n), dtype=float)\n",
    "h_b = numpy.arange(numpy.multiply(*d_a.shape), dtype=float).reshape(d_a.shape)\n",
    "\n",
    "d_a.set(h_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07370b77",
   "metadata": {},
   "source": [
    "#### Moving data to CPU\n",
    "\n",
    "* `cupy.asnumpy` method returns a NumPy array created based on the provided input. This method accepts CuPy arrays, but not only.\n",
    "* `cupy.ndarray.get` method returns a NumPy array that corresponds to the CuPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a759ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data = cp.array([1, 2, 3])\n",
    "h_data = cp.asnumpy(d_data)\n",
    "\n",
    "# Alternative:\n",
    "h_data_too = d_data.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55118a2",
   "metadata": {},
   "source": [
    "### Memory management\n",
    "\n",
    "In general, CuPy takes care of memory issues in the background. What we need to know about memory management in CuPy, is that to mitigate overheads associated with memory allocation and CPU/GPU synchronization, CuPy uses two _memory pools_:\n",
    "\n",
    "* Device memory pool. Used for GPU memory allocations.\n",
    "* Pinned memory pool. Used during CPU-to-GPU data transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60fffb",
   "metadata": {},
   "source": [
    "### User-defined kernels\n",
    "\n",
    "Similar to PyCUDA, CuPy allows a programmer to process data by means of three types of kernels:\n",
    "\n",
    "1. Elementwise kernels\n",
    "2. Reduction kernels\n",
    "3. Custom kernels, called _raw_ in CuPy nomenclature.\n",
    "\n",
    "Luckily, CuPy's kernel syntax is somewhat similar to that of PyCUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89bfc9",
   "metadata": {},
   "source": [
    "### Elementwise kernels\n",
    "\n",
    "Here is an example of a kernel that computes elementwise squared difference for two arrays `x` and `y`:\n",
    "\n",
    "```python\n",
    "squared_diff = cupy.ElementwiseKernel(\n",
    "   'float32 x, float32 y',\n",
    "   'float32 z',\n",
    "   'z = (x - y) * (x - y)',\n",
    "   'squared_diff')\n",
    "```\n",
    "The first argument is a string representation of comma-separated input arguments.\n",
    "The second argument is a string representation of the (internal) output variable.\n",
    "The third argument is a string representation of the body of the kernel.\n",
    "The last argument is the name of the kernel.\n",
    "\n",
    "Once the kernel is compiled, it can be used in Python code as a normal Python function:\n",
    "\n",
    "```python\n",
    "x = cupy.arange(10, dtype=numpy.float32).reshape(2, 5)\n",
    "y = cupy.arange(5, dtype=numpy.float32)\n",
    "squared_diff(x, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a26099",
   "metadata": {},
   "source": [
    "### Reduction kernels\n",
    "\n",
    "Here is an example of a custom reduction kernel that computes L2 norm along specified axis:\n",
    "\n",
    "```python\n",
    "l2norm_kernel = cupy.ReductionKernel(\n",
    "    'T x',  # input params\n",
    "    'T y',  # output params\n",
    "    'x * x',  # map\n",
    "    'a + b',  # reduce. 'a' and 'b' are reserved variables\n",
    "    'y = sqrt(a)',  # post-reduction map. 'a' is a reserved variable. 'y' is the output param above \n",
    "    '0',  # identity value (that is, axis)\n",
    "    'l2norm'  # kernel name\n",
    ")\n",
    "```\n",
    "which can be used like a normal Python function applied to a CuPy array:\n",
    "```python\n",
    "d_data = cupy.arange(10, dtype=numpy.float32).reshape(2, 5)\n",
    "l2norm_kernel(d_data, axis=1)\n",
    "```\n",
    "Detailed discussion of reduction kernels is beyond the scope of this brief overview. If you're interested in reduction kernels, navigate to the corresponding page of the User's Guide: https://docs.cupy.dev/en/stable/user_guide/kernel.html#reduction-kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5835aae",
   "metadata": {},
   "source": [
    "### Raw kernels\n",
    "\n",
    "CuPy provides a mechanism to create individual kernels in CUDA C. This approach enables fine-grained control over kernel execution parameters. Here is an example of a raw kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a50549",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kernel = cupy.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void my_add(const float* x1, const float* x2, float* y) {\n",
    "    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    y[idx] = x1[idx] + x2[idx];\n",
    "}\n",
    "''', 'my_add')\n",
    "\n",
    "x1 = cupy.arange(25, dtype=cupy.float32).reshape(5, 5)\n",
    "x2 = cupy.arange(25, dtype=cupy.float32).reshape(5, 5)\n",
    "y = cupy.zeros((5, 5), dtype=cupy.float32)\n",
    "\n",
    "add_kernel((5,), (5,), (x1, x2, y))  # 5x1x1 grid,  5x1x1 blocks, and arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfedbd9",
   "metadata": {},
   "source": [
    "### Raw modules\n",
    "\n",
    "Raw modules encapsulate several Raw kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49518b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_from_source = r'''\n",
    "extern \"C\"{\n",
    "__global__ void test_sum(const float* x1, const float* x2, float* y, unsigned int N) {\n",
    "    unsigned int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    if (idx < N)\n",
    "        y[idx] = x1[idx] + x2[idx];\n",
    "}\n",
    "\n",
    "__global__ void test_multiply(const float* x1, const float* x2, float* y, unsigned int N){\n",
    "    unsigned int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    if (idx < N)\n",
    "        y[idx] = x1[idx] * x2[idx];\n",
    "}\n",
    "}'''\n",
    "module = cupy.RawModule(code=loaded_from_source)\n",
    "ker_sum = module.get_function('test_sum')\n",
    "ker_times = module.get_function('test_multiply')\n",
    "\n",
    "# generate some data\n",
    "N = 10\n",
    "x1 = cupy.arange(N**2, dtype=cupy.float32).reshape(N, N)\n",
    "x2 = cupy.ones((N, N), dtype=cupy.float32)\n",
    "y = cupy.zeros((N, N), dtype=cupy.float32)\n",
    "\n",
    "# apply 'test_sum' kernel\n",
    "ker_sum((N,), (N,), (x1, x2, y, N**2))   # y = x1 + x2\n",
    "assert cupy.allclose(y, x1 + x2)\n",
    "\n",
    "# apply 'test_multiply' kernel\n",
    "ker_times((N,), (N,), (x1, x2, y, N**2)) # y = x1 * x2\n",
    "assert cupy.allclose(y, x1 * x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ab8ac",
   "metadata": {},
   "source": [
    "## Better kernels\n",
    "\n",
    "Simple elementwise and reduction kernels can also be defined more easily using the `cupy.fuse()` decorator. For example, the `squared_diff` kernel that we defined in the \"Elementwise kernels\" section can be created with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7268a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cupy.fuse()\n",
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3264be0",
   "metadata": {},
   "source": [
    "And here is an example of a simple reduction kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cupyp.fuse()\n",
    "def sum_of_products(x, y):\n",
    "    return cupy.sum(x * y, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38091e0",
   "metadata": {},
   "source": [
    "These kernels can be called on CuPy arrays, NumPy arrays, and even scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46851e14",
   "metadata": {},
   "source": [
    "## JIT Raw kernels\n",
    "\n",
    "Finally, CuPy provides a way to use the decorator approach to create Raw kernels! For that, we need `jit` module from the `cupyx` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupyx\n",
    "\n",
    "@cupyx.jit.rawkernel()\n",
    "def elementwise_copy(x, y, size):\n",
    "    idx = jit.threadIdx.x + jit.blockIdx.x * jit.blockDim.x\n",
    "    ntid = jit.gridDim.x * jit.blockDim.x\n",
    "    for i in range(idx, size, ntid):\n",
    "        y[i] = x[i]\n",
    "\n",
    "# How to use\n",
    "size = cupy.uint32(2 ** 22)\n",
    "x = cupy.random.normal(size=(size,), dtype=cupy.float32)\n",
    "y = cupy.empty((size,), dtype=cupy.float32)\n",
    "\n",
    "elementwise_copy((128,), (1024,), (x, y, size))  # RawKernel style\n",
    "assert (x == y).all()\n",
    "\n",
    "elementwise_copy[128, 1024](x, y, size)  #  Numba style\n",
    "assert (x == y).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
