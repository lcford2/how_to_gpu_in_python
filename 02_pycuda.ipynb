{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9302bed1",
   "metadata": {},
   "source": [
    "## PyCUDA\n",
    "\n",
    "PyCUDA is a Python package that provides a lean interface to CUDA API and CUDA C. It introduces a minimal amount of new concepts and thus is a good choice for programmers familiar with Python and CUDA. In this section we will review the major components of PyCUDA and how one can use them to write CUDA applications in Python.\n",
    "\n",
    "**GitHub**: https://github.com/inducer/pycuda<br />\n",
    "**Documentation**: https://documen.tician.de/pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf24205",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "Let's begin by reviewing a PyCUDA application that multiplies elements of a floating-point array by a specified factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4770f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyCUDA\n",
    "# Common for many PyCUDA programms\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver\n",
    "import pycuda.compiler\n",
    "\n",
    "import numpy\n",
    "\n",
    "mod = pycuda.compiler.SourceModule(\"\"\"\n",
    "__global__  void  double_array (float* d_a, unsigned int n_elements, float factor) {\n",
    "    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n_elements)\n",
    "        d_a[idx] *= factor;\n",
    "}\"\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Input parameters. Feel free to change them\n",
    "    data_size = 1<<20 # 1048576\n",
    "    factor = 10 * numpy.random.random()\n",
    "    \n",
    "    # Initialize array on host\n",
    "    h_array = numpy.random.uniform(0, 50, size=data_size)\n",
    "    \n",
    "    # Convert to 32-bit floating-point\n",
    "    h_array = h_array.astype(numpy.float32)\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_array = pycuda.driver.mem_alloc(h_array.nbytes)\n",
    "    \n",
    "    # Copy host array to device memory\n",
    "    pycuda.driver.memcpy_htod(d_array, h_array)\n",
    "    \n",
    "    # Find reference to the function\n",
    "    multiplication_kernel = mod.get_function('double_array')\n",
    "    \n",
    "    # Set block and grid dimensions\n",
    "    blockDim = (128, 1, 1)\n",
    "    grid_size_x = (data_size + blockDim[0] - 1) // blockDim[0]\n",
    "    gridDim = (grid_size_x, 1, 1)\n",
    "    \n",
    "    # Launch the kernel\n",
    "    multiplication_kernel(d_array, numpy.uint32(data_size), numpy.float32(factor), grid=gridDim, block=blockDim)\n",
    "    \n",
    "    # Transfer the results back to host\n",
    "    results = numpy.empty_like(h_array)\n",
    "    pycuda.driver.memcpy_dtoh(results, d_array)\n",
    "    \n",
    "    # Release GPU memory\n",
    "    d_array.free()\n",
    "    \n",
    "    # Make sure the results are correct\n",
    "    assert numpy.allclose(results, h_array * factor), \"Failed\"\n",
    "    \n",
    "    # Report success\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1096385",
   "metadata": {},
   "source": [
    "This is great but even though we didn't have to specify such things as directions of memory copy and compute the size of transferred data, this isn't very exciting, because we still had to not only write a CUDA kernel in CUDA C, but also perform all the memory and data manipulation steps typical for a CUDA C program.\n",
    "\n",
    "PyCUDA provides a few ways to simplify the above program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c832c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyCUDA\n",
    "# Common for many PyCUDA programms\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver\n",
    "import pycuda.compiler\n",
    "\n",
    "import numpy\n",
    "\n",
    "mod = pycuda.compiler.SourceModule(\"\"\"\n",
    "__global__  void  double_array (float* d_a, unsigned int n_elements, float factor) {\n",
    "    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n_elements)\n",
    "        d_a[idx] *= factor;\n",
    "}\"\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Input parameters. Feel free to change them\n",
    "    data_size = 1<<20 # 1048576\n",
    "    factor = 10 * numpy.random.random()\n",
    "    \n",
    "    # Initialize array on host\n",
    "    h_array = numpy.random.uniform(0, 50, size=data_size)\n",
    "    \n",
    "    # Convert to 32-bit floating-point\n",
    "    h_array = h_array.astype(numpy.float32)\n",
    "    \n",
    "    # Save a copy to verify the results\n",
    "    h_array_copy = h_array.copy()\n",
    "\n",
    "    # Find reference to the function\n",
    "    multiplication_kernel = mod.get_function('double_array')\n",
    "\n",
    "    # Set block and grid dimensions\n",
    "    blockDim = (128, 1, 1)\n",
    "    grid_size_x = (data_size + blockDim[0] - 1) // blockDim[0]\n",
    "    gridDim = (grid_size_x, 1, 1)\n",
    "      \n",
    "    # Launch the kernel\n",
    "    multiplication_kernel(pycuda.driver.InOut(h_array), numpy.uint32(data_size), numpy.float32(factor), grid=gridDim, block=blockDim)\n",
    "    \n",
    "    # Make sure the results are correct\n",
    "    assert numpy.allclose(h_array, h_array_copy * factor), \"Failed\"\n",
    "    \n",
    "    # Report success\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b5aa1",
   "metadata": {},
   "source": [
    "That's a little bit better! Here we used `pycuda.driver.InOut` convenience function which tells PyCUDA to copy  provided array to the GPU, do the calculations, transfer the results back to the host and save them to the same array, essentially, overwriting input. Thanks to `pycuda.driver.InOut`, we did not have to allocate memory on the device, transfer the data from host to gpu and back, and we didn't have to deallocate the GPU array.\n",
    "\n",
    "If, instead, we wrote a different kernel that had input and output separated like so:\n",
    "\n",
    "```\n",
    "mod = pycuda.compiler.SourceModule(\"\"\"\n",
    "__global__  void  double_array (float* input, float *output, unsigned int n_elements, float factor) {\n",
    "    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n_elements)\n",
    "        output[idx] = input[idx] * factor;\n",
    "}\"\"\")\n",
    "```\n",
    "\n",
    "we could then use another two conveniece functions: `pycuda.driver.In` and `pycuda.driver.Out` to send data to and from the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#... \n",
    "    # Placeholder for the final result\n",
    "    results = numpy.empty_like(h_array)\n",
    "\n",
    "#...      \n",
    "    # Launch the kernel\n",
    "    multiplication_kernel(pycuda.driver.In(h_array), pycuda.driver.Out(results), numpy.uint32(data_size), numpy.float32(factor), grid=gridDim, block=blockDim)\n",
    "    \n",
    "    # Make sure the results are correct\n",
    "    assert numpy.allclose(results, h_array * factor), \"Failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b54e2b",
   "metadata": {},
   "source": [
    "Essentially, both `pycuda.driver.In` and `pycuda.driver.Out` save us from manually allocating memory on the device, transferring data from host to device and back and then deallocating memory! Very convenient!\n",
    "\n",
    "### But wait, there's more!\n",
    "\n",
    "For such applications PyCUDA provides another convenience interface - `pycuda.gpuarray.GPUArray`, which associates Numpy arrays with arrays on the device, handles transfers, and allows the programmer to express array operations with Numpy syntax. For example, the above program can be slimmed down to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray\n",
    "\n",
    "import numpy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Input parameters. Feel free to change them\n",
    "    data_size = 1<<20 # 1048576\n",
    "    factor = 10 * numpy.random.random()\n",
    "    \n",
    "    # Initialize array on host\n",
    "    h_array = numpy.random.uniform(0, 50, size=data_size)\n",
    "    \n",
    "    # Convert to 32-bit floating-point\n",
    "    h_array = h_array.astype(numpy.float32)\n",
    "    \n",
    "    # Associate h_array with an array on a GPU and do the calculations\n",
    "    d_gpuarray = pycuda.gpuarray.to_gpu(h_array)\n",
    "    d_results = factor * d_gpuarray\n",
    "\n",
    "    # Copy the result to CPU using .get() method\n",
    "    results = d_results.get()\n",
    "    \n",
    "    # Make sure the results are correct\n",
    "    assert numpy.allclose(results, h_array * factor), \"Failed\"\n",
    "    \n",
    "    # Report success\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4b181",
   "metadata": {},
   "source": [
    "## Element-wise operations\n",
    "\n",
    "To do a bit more complex mathematical operations on arrays, we can use the `pycuda.cumath` module, which provides such functions as `sqrt`, `sin`, `cos`, `log`, `exp`, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray\n",
    "\n",
    "import numpy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    h_array = numpy.random.randn(4000,4000).astype(numpy.float32)\n",
    "    d_gpuarray = pycuda.gpuarray.to_gpu(h_array)\n",
    "    \n",
    "    # Compute \n",
    "    result = pycuda.cumath.sqrt(pycuda.cumath.fabs(d_gpuarray)).get()\n",
    "    assert numpy.allclose(numpy.sqrt(numpy.abs(h_array)), result), \"Failed\"\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d4914",
   "metadata": {},
   "source": [
    "PyCUDA also provides a module for custom element-wise operations, `pycuda.elementwise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray\n",
    "import pycuda.elementwise\n",
    "import pycuda.curandom\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Generate random PyCUDA arrays on GPU\n",
    "    a_gpu = pycuda.curandom.rand((50,))\n",
    "    b_gpu = pycuda.curandom.rand((50,))\n",
    "\n",
    "    # Set up an element-wise operation\n",
    "    lin_comb = pycuda.elementwise.ElementwiseKernel(\n",
    "            \"float a, float *x, float b, float *y, float *z\",\n",
    "            \"z[i] = a*x[i] + b*y[i]\",\n",
    "            \"linear_combination\")\n",
    "\n",
    "    c_gpu = pycuda.gpuarray.empty_like(a_gpu)\n",
    "    lin_comb(5, a_gpu, 6, b_gpu, c_gpu)\n",
    "    \n",
    "    print(c_gpu.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c316732",
   "metadata": {},
   "source": [
    "## Reductions\n",
    "\n",
    "Finding a reduced value for an array on a GPU is a well-known pain point: as you remember, threads on a GPU device don't know much about each other unless they're in the same \"block\". PyCUDA provides several convenience methods that compute several reductions on GPU arrays: sum, dot product with another matrix, max, min, as well as versions of these reductions but applied to subsets of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray\n",
    "import pycuda.curandom\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Generate random PyCUDA arrays on GPU\n",
    "    a_gpu = pycuda.curandom.rand((50,))\n",
    "    b_gpu = pycuda.curandom.rand((50,))\n",
    "    \n",
    "    maximum = pycuda.gpuarray.max(a_gpu).get()\n",
    "    dot_product = pycuda.gpuarray.dot(a_gpu, b_gpu).get()\n",
    "    \n",
    "    print(\"max(a)    = \", maximum)\n",
    "    print(\"dot(a, b) = \", dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720ab40",
   "metadata": {},
   "source": [
    "Lastly, similar to custom element-wise operations, PyCUDA provides a mechanism for custom reductions, though this topic is beyond the scope of this overview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09519a91",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. PyCUDA documentation: https://documen.tician.de/pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70663d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
