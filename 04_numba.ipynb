{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numba.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idpNYCcjL3XA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqdo13bAEywJ"
      },
      "source": [
        "# Numba Overview\n",
        "\t\n",
        "\n",
        "*   Just-in-time (JIT) compiler for Python\n",
        "*   Specify functions to be compiled using decorators: `@<decorator> def func_name ...`\n",
        "*   CPU usage: Decorated functions compiled once before first execution and then all executions performed at machine code speed\n",
        "*   GPU usage: Decorated functions compiled into CUDA kernels and device functions for running on a GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dobp1CXyFyxD"
      },
      "source": [
        "# Using Numba to Program GPUs\n",
        "\n",
        "## Requirements\n",
        "\t\n",
        "\n",
        "*   Numba package available via `conda install numba` and `pip install numba` (and compiling from source)\n",
        "*   Requires CUDA-enabled GPU with compute capability 2.0+, up-to-date Nvidia driver, and cudatoolkit package\n",
        " *   cudatoolkit available via `conda install cudatoolkit` for Anaconda\n",
        " *   Other Numba installations require separate installation of CUDA Toolkit (e.g., via CUDA SDK installer)\n",
        "\n",
        "## How to use it\n",
        "*   Write a kernel in Python - one must stick to a subset of supported Python code (e.g., exception handling via `try ... except` is not supported)\n",
        "*   Mark the function with a Numba CUDA decorator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSWOL3cWG5c9"
      },
      "source": [
        "from numba import cuda\n",
        "@cuda.jit\n",
        "def my_kernel(...):\n",
        "\t..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83rGpvVMHfN_"
      },
      "source": [
        "*\tNumba compiles it into a CUDA kernel\n",
        "*\tNumba CUDA kernels interface directly with NumPy arrays, which are transferred automatically between the CPU and GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL8XcuLXHrNo"
      },
      "source": [
        "## Advantages and disadvantages\n",
        "* Advantages: Pure Python code; avoids some low-level hassles of CUDA\n",
        "* Disadvantages: Numba offers only a subset of all CUDA capabilities (e.g., dynamic parallelism and texture memory are not available yet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE1NicYzIBs7"
      },
      "source": [
        "## Writing and executing CUDA kernels\n",
        "*\tAs with CUDA programming, need to think in terms of grids/blocks/threads and global/shared/local GPU memory\n",
        "*\tKernels require the number of blocks per grid and the number of threads per block when called"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufPg3CNPII_S"
      },
      "source": [
        "threadsperblock = 32\n",
        "blockspergrid = math.ceil(my_array.size / threadsperblock)\n",
        "increment_by_one[blockspergrid, threadsperblock](an_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wypkj5kMIKNz"
      },
      "source": [
        "*\tCUDA objects used for accessing the dimensions of the thread hierarchy\n",
        " *\t\t`numba.cuda.threadIdx` - thread ID\n",
        " *\t\t`numba.cuda.blockIdx` - block ID\n",
        " *\t\t`numba.cuda.blockDim` - number of threads per block\n",
        " *\t\t`numba.cuda.gridDim` - total number of blocks\n",
        " *\t\t`numba.cuda.grid(ndim)` - absolute position of the current thread in the entire grid of blocks (ndim - number of dimensions declared when instantiating the kernel)\n",
        " *\t\t`numba.cuda.gridsize(ndim)` - absolute size/shape in threads of entire grid of blocks  (ndim - number of dimensions declared when instantiating the kernel)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW9TfWEnH4cg",
        "outputId": "cb5d4ad0-50d8-4e4e-ac3e-80e76b9716ef"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    pos = cuda.grid(1)\n",
        "    if pos < an_array.size:\n",
        "        an_array[pos] += 1\n",
        "\n",
        "my_array = np.array([3, 5, 9, 13, 7, 2, 4, 1])\n",
        "\n",
        "threadsperblock = 4\n",
        "blockspergrid = math.ceil(my_array.size / threadsperblock)\n",
        "increment_by_one[blockspergrid, threadsperblock](my_array)\n",
        "\n",
        "print(threadsperblock)\n",
        "print(blockspergrid)\n",
        "print(my_array)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "2\n",
            "[ 4  6 10 14  8  3  5  2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3y3hSRQEu6_"
      },
      "source": [
        "## Memory Management in Numba\n",
        "* Automatically transfers NumPy arrays to the device\n",
        "* By default, Numba always transfers device memory back to the host when a kernel finishes\n",
        "* Manual controls for avoiding unnecessary transfer of read-only arrays\n",
        " *\t`numba.cuda.device_array(shape, dtype=np.float, strides=None, order='C', stream=0)` - allocate empty device ndarray (like numpy.empty())\n",
        " *\t`numba.cuda.device_array_like(ary, stream=0)` - same as device_array() except it uses information from the array\n",
        " *\t`numba.cuda.to_device(obj, stream=0, copy=True, to=None)` - allocate and transfer NumPy ndarray to device\n",
        "   *\t\tFor example, copying a NumPy array from host to device:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BORpxVIIwBC"
      },
      "source": [
        "ary = np.arange(10)\n",
        "d_ary = cuda.to_device(ary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raRXcnszI3n0"
      },
      "source": [
        "* Also support for:\n",
        " *\tUsing streams to transfer arrays\n",
        " *\tAccessing device arrays\n",
        " *\tPinned memory\n",
        " *\tMapped memory\n",
        " *\tAccessing global, shared, and local device memory\n",
        "\n",
        "* Numba queues up device memory deallocations when no more references exist\n",
        " *\tDeallocations can be paused temporarily if necessary with `numba.cuda.defer_cleanup()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NycFlWFgJJRx"
      },
      "source": [
        "## Reductions\n",
        "* Numba reduction support is simpler than with CUDA, using a `@cuda.reduce` decorator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmcoKXBdJQPO"
      },
      "source": [
        "@cuda.reduce\n",
        "def sum_reduce(a, b):\n",
        "    return a + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU7BOHJ8LT_t"
      },
      "source": [
        "# References\n",
        "* Numba documentation: https://numba.pydata.org/numba-doc/dev/index.html"
      ]
    }
  ]
}